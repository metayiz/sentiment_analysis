# -*- coding: utf-8 -*-
"""Sentiment Analysis with Tfidf - PROJECT 360

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10eKKYH1FJ_62QC685JQCDDth_2qQPqYn
"""

# Download dataset (you can also use the file in the zip if you are offline)

! wget https://raw.githubusercontent.com/SK7here/Movie-Review-Sentiment-Analysis/master/IMDB-Dataset.csv

# Read dataset
import pandas as pd

f = open("IMDB-Dataset.csv")
data = pd.read_csv(f, nrows=30000)

data.head()

data.tail()

data.shape
data.describe

# Drop duplicate reviews

data = data.drop_duplicates(subset="review")
data.describe

data.sentiment.value_counts()

import matplotlib.pyplot as plt

data.sentiment.value_counts().plot(kind='pie',autopct='%.2f')
plt.show()

from wordcloud import WordCloud

review_text = ' '.join(data['review'].astype(str))
cloud_data = WordCloud(background_color="white").generate(review_text)
plt.imshow(cloud_data, interpolation='bilinear') 
plt.axis("off")
plt.show()

from wordcloud import STOPWORDS

my_stopwords = set(STOPWORDS)
my_stopwords.update(["movie", "movies", "film", "films", "watch", "br"])

my_cloud = WordCloud(background_color = "white", stopwords=my_stopwords).generate(review_text)
plt.imshow(my_cloud, interpolation = "bilinear")

# Convert sentiment to numbers

label_to_int = {'negative':0,'positive':1}
int_to_label = {0:'negative',1:'positive'}

data['label'] = data.sentiment.map(label_to_int)

data.head()
data.tail()

import nltk 
import re
from nltk.corpus import stopwords
nltk.download('stopwords')

lemmatizer = nltk.stem.WordNetLemmatizer()
stopwords = set(stopwords.words('english'))

def PreprocessReviews(text):
  #html tags:
  text = re.sub('<.*?>',' ',text)
  #special characters (punctuation) '@,!' e.t.c.
  text = re.sub('\W',' ',text)
  #single characters
  text = re.sub('\s+[a-zA-Z]\s+',' ', text)
  #multiple spaces with single space
  text = re.sub('\s+',' ', text)
  tokenized_text = nltk.word_tokenize(text)
  clean_text = [word for word in tokenized_text if word not in my_stopwords]
  return clean_text

import nltk
nltk.download('punkt')

test_review = "Wow! THIS is the best movie I have ever seen !!!!"

PreprocessReviews(test_review)

# Preprocess reviews

data.loc[:, "review"] = data["review"].apply(lambda x: PreprocessReviews(x))

data.head()

# Train-test split

from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(data['review'],data['sentiment'])

print("Total Data: {}".format(data.shape[0]))
print("Training Data: {}".format(X_train.shape[0]))
print("Test Data: {}".format(X_test.shape[0]))

y_train[:10]

# Create TF-IDF vectorizer

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x: x, ngram_range=(1, 2))

# Transform training data

tfidf_train = tfidf_vectorizer.fit_transform(X_train)

print('vocabulary size: {}'.format(len(tfidf_vectorizer.get_feature_names_out())))
vocabulary = tfidf_vectorizer.get_feature_names_out() 
print(vocabulary[15000:15100])

import numpy as np
np.set_printoptions(threshold=np.inf) 
doc_1 = tfidf_train.toarray()[0]
print(doc_1)

vocab_indices = []

for i in range(len(doc_1)):
  if doc_1[i] != 0:  
    vocab_indices.append(i)

for index in vocab_indices:
  print(tfidf_vectorizer.get_feature_names_out()[index])

# Transform test data

tfidf_test = tfidf_vectorizer.transform(X_test)

test_1 = tfidf_test.toarray()[0]
test_indices = []

for i in range(len(test_1)):
  if test_1[i] != 0:  
    test_indices.append(i)

for index in test_indices:
  print(tfidf_vectorizer.get_feature_names_out()[index])

# Create Naive Bayes classifier

from sklearn.naive_bayes import MultinomialNB
naive_bayes = MultinomialNB()
naive_bayes.fit(tfidf_train, y_train)

# Make predictions

predictions = naive_bayes.predict(tfidf_test)

len(predictions)

# Evaluate model performance

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('Accuracy score: ', format(accuracy_score(y_test, predictions,)))
print('Precision score: ', format(precision_score(y_test, predictions, average="weighted")))
print('Recall score: ', format(recall_score(y_test, predictions, average="weighted")))
print('F1 score: ', format(f1_score(y_test, predictions, average="weighted")))

# Example usage

sample_review = [PreprocessReviews("It is a good movie")]

sample_review_processed = tfidf_vectorizer.transform(sample_review)
prediction = naive_bayes.predict(sample_review_processed)
print("The sentiment of the review is:", prediction[0])